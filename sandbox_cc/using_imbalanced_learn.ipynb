{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    " \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    " \n",
    "def test_gini():\n",
    "    def fequ(a,b):\n",
    "        return abs( a -b) < 1e-6\n",
    "    def T(a, p, g, n):\n",
    "        assert( fequ(gini(a,p), g) )\n",
    "        assert( fequ(gini_normalized(a,p), n) )\n",
    "    T([1, 2, 3], [10, 20, 30], 0.111111, 1)\n",
    "    T([1, 2, 3], [30, 20, 10], -0.111111, -1)\n",
    "    T([1, 2, 3], [0, 0, 0], -0.111111, -1)\n",
    "    T([3, 2, 1], [0, 0, 0], 0.111111, 1)\n",
    "    T([1, 2, 4, 3], [0, 0, 0, 0], -0.1, -0.8)\n",
    "    T([2, 1, 4, 3], [0, 0, 2, 1], 0.125, 1)\n",
    "    T([0, 20, 40, 0, 10], [40, 40, 10, 5, 5], 0, 0)\n",
    "    T([40, 0, 20, 0, 10], [1000000, 40, 40, 5, 5], 0.171428,\n",
    "      0.6)\n",
    "    T([40, 20, 10, 0, 0], [40, 20, 10, 0, 0], 0.285714, 1)\n",
    "    T([1, 1, 0, 1], [0.86, 0.26, 0.52, 0.32], -0.041666,\n",
    "      -0.333333)\n",
    "    \n",
    "test_gini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    573518\n",
       "1     21694\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'target', u'ps_ind_01', u'ps_ind_02_cat', u'ps_ind_03',\n",
       "       u'ps_ind_04_cat', u'ps_ind_05_cat', u'ps_ind_06_bin', u'ps_ind_07_bin',\n",
       "       u'ps_ind_08_bin', u'ps_ind_09_bin', u'ps_ind_10_bin', u'ps_ind_11_bin',\n",
       "       u'ps_ind_12_bin', u'ps_ind_13_bin', u'ps_ind_14', u'ps_ind_15',\n",
       "       u'ps_ind_16_bin', u'ps_ind_17_bin', u'ps_ind_18_bin', u'ps_reg_01',\n",
       "       u'ps_reg_02', u'ps_reg_03', u'ps_car_01_cat', u'ps_car_02_cat',\n",
       "       u'ps_car_03_cat', u'ps_car_04_cat', u'ps_car_05_cat', u'ps_car_06_cat',\n",
       "       u'ps_car_07_cat', u'ps_car_08_cat', u'ps_car_09_cat', u'ps_car_10_cat',\n",
       "       u'ps_car_11_cat', u'ps_car_11', u'ps_car_12', u'ps_car_13',\n",
       "       u'ps_car_14', u'ps_car_15', u'ps_calc_01', u'ps_calc_02', u'ps_calc_03',\n",
       "       u'ps_calc_04', u'ps_calc_05', u'ps_calc_06', u'ps_calc_07',\n",
       "       u'ps_calc_08', u'ps_calc_09', u'ps_calc_10', u'ps_calc_11',\n",
       "       u'ps_calc_12', u'ps_calc_13', u'ps_calc_14', u'ps_calc_15_bin',\n",
       "       u'ps_calc_16_bin', u'ps_calc_17_bin', u'ps_calc_18_bin',\n",
       "       u'ps_calc_19_bin', u'ps_calc_20_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Default handling nan **\n",
    "\n",
    "For now, just use columns distribution to fill in empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ind_02_cat\n",
      "ps_ind_04_cat\n",
      "ps_ind_05_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ubuntu_ve_2.9/lib/python2.7/site-packages/pandas/core/indexing.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_reg_03\n",
      "ps_car_01_cat\n",
      "ps_car_02_cat\n",
      "ps_car_03_cat\n",
      "ps_car_05_cat\n",
      "ps_car_07_cat\n",
      "ps_car_09_cat\n",
      "ps_car_11\n",
      "ps_car_12\n",
      "ps_car_14\n"
     ]
    }
   ],
   "source": [
    "df_train.replace(-1, np.nan, inplace=True)\n",
    "null_vals = df_train.isnull().sum(axis=0)[df_train.isnull().sum(axis=0)>0]\n",
    "value_count_columns = {i: df_train[i].value_counts(1) for i in df_train.columns}    \n",
    "df_train_copy = df_train.copy()\n",
    "for c in df_train.columns:\n",
    "    total_num = df_train[c].isnull().sum()\n",
    "    if(total_num == 0):\n",
    "        continue\n",
    "    print(c)\n",
    "    \n",
    "    random_vals = np.random.choice(list(value_count_columns[c].index), total_num, list(value_count_columns[c].values))\n",
    "    \n",
    "    df_train_copy[c].loc[df_train[c].isnull()] = random_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test full dataset without subsampling **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595212, 58)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dist of 0/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    573518\n",
       "1     21694\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Current dist of 0/1')\n",
    "df_train_copy['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('potentially_unnecessary_features.txt') as r:\n",
    "    features_to_remove = [l.strip() for l in r]\n",
    "\n",
    "cols_for_random_forest = list(set(df_train_copy.columns) - set(features_to_remove + ['target']))\n",
    "\n",
    "cols_for_lr = [\n",
    "    'ps_ind_16_bin', 'ps_ind_09_bin', 'ps_ind_01', 'ps_ind_08_bin', 'ps_ind_06_bin', \n",
    "    'ps_car_12', 'ps_ind_17_bin', 'ps_calc_02', 'ps_calc_03', 'ps_calc_01', 'ps_reg_02',\n",
    "    'ps_reg_03', 'ps_reg_01', 'ps_ind_04_cat', 'ps_car_07_cat', 'ps_car_02_cat', 'ps_car_11',\n",
    "    'ps_car_13', 'ps_car_15', 'ps_car_14', 'ps_ind_07_bin', 'ps_ind_02_cat', 'ps_ind_15', 'ps_car_09_cat', \n",
    "    'ps_car_08_cat', 'ps_ind_05_cat', 'ps_ind_18_bin'\n",
    "]\n",
    "\n",
    "cols_for_bc = [\n",
    "    u'ps_ind_09_bin', u'ps_ind_01', u'ps_ind_03', u'ps_ind_08_bin',\n",
    "    u'ps_ind_06_bin', u'ps_car_12', u'ps_ind_17_bin', u'ps_calc_03',\n",
    "    u'ps_reg_02', u'ps_reg_03', u'ps_calc_04', u'ps_reg_01',\n",
    "    u'ps_ind_04_cat', u'ps_calc_09', u'ps_car_07_cat', u'ps_car_02_cat',\n",
    "    u'ps_car_13', u'ps_ind_07_bin', u'ps_ind_02_cat', u'ps_ind_15',\n",
    "    u'ps_car_09_cat', u'ps_calc_07', u'ps_calc_13', u'ps_calc_14',\n",
    "    u'ps_car_06_cat', u'ps_calc_05', u'ps_car_08_cat', u'ps_ind_05_cat',\n",
    "    u'ps_ind_18_bin'\n",
    "]\n",
    "\n",
    "all_cols = list(set(df_train_copy.columns) - set(['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtractFeatures(sklearn.base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    simply remove the sets of features we want to use for a specific model\n",
    "    \"\"\"\n",
    "    def __init__(self, cols=[], df_col_names=None):\n",
    "        \"\"\"\n",
    "        define which columns to extract from dataframe\n",
    "        \"\"\"        \n",
    "        self.use_idx_vals = False\n",
    "        self.use_cols = cols\n",
    "        if not (df_col_names is None):\n",
    "            self.use_cols = [i for i, c in enumerate(df_col_names) if c in cols]                \n",
    "            self.use_idx_vals = True\n",
    "    \n",
    "    #def get_params(self, deep=True):\n",
    "    #    return {\n",
    "    #        'cols': getattr(self, 'use_cols', None)\n",
    "    #    }\n",
    "    \n",
    "    def dummy_extract(self, dfX):\n",
    "        \"\"\"\n",
    "        extract cols and return as np.array\n",
    "        \"\"\"\n",
    "        if self.use_idx_vals:\n",
    "            if isinstance(dfX, pd.DataFrame):\n",
    "                return dfX.values[:, self.use_cols]\n",
    "            else:\n",
    "                return dfX[:, self.use_cols]\n",
    "        else:\n",
    "            return dfX[self.use_cols].values\n",
    "    def transform(self, dfX):\n",
    "        return self.dummy_extract(dfX)\n",
    "    def fit(self, dfX, _y):\n",
    "        self.dummy_extract(dfX)\n",
    "        return self\n",
    "\n",
    "def TestExtract():\n",
    "    a = pd.DataFrame({\n",
    "        'a': ['a', 'b', 'c'],\n",
    "        'b': ['d', 'e', 'f'],\n",
    "        'c': ['g', 'h', 'i'],\n",
    "    })\n",
    "    col = ['a', 'c']\n",
    "    assert ((a[col].values != ExtractFeatures(col).transform(a)).sum()==0)\n",
    "    col = ['a']\n",
    "    assert ((a[col].values != ExtractFeatures(col).transform(a)).sum()==0)\n",
    "    col = ['c']\n",
    "    assert ((a[col].values != ExtractFeatures(col).transform(a)).sum()==0)    \n",
    "    assert ((a[col].values != ExtractFeatures(col, a.columns).transform(a)).sum()==0)\n",
    "    assert ((a[col].values != ExtractFeatures(col, a.columns).transform(a.values)).sum()==0)\n",
    "\n",
    "TestExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'SMOTE')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ubuntu_ve_2.9/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'SMOTE')\n",
      "(2, 'SMOTE')\n",
      "(3, 'SMOTE')\n",
      "(4, 'SMOTE')\n",
      "(5, 'SMOTE')\n",
      "(6, 'SMOTE')\n",
      "(7, 'SMOTE')\n",
      "(8, 'SMOTE')\n",
      "(9, 'SMOTE')\n"
     ]
    }
   ],
   "source": [
    "p1 = make_pipeline(\n",
    "    SMOTE(kind='borderline1'),\n",
    "    ExtractFeatures(cols_for_lr, df_train_copy.columns),\n",
    "    StandardScaler(),\n",
    "    # ExtractFeatures(cols_for_random_forest, X_train.columns),\n",
    "    # RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=None, n_jobs=4)\n",
    "    LogisticRegression(n_jobs=-1)\n",
    ")\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, 'SMOTE')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_train_copy, df_train_copy['target'], test_size=0.2\n",
    "    )\n",
    "    p1.fit(X_train, y_train)\n",
    "    acc.append(gini_normalized(y_test, p1.predict_proba(X_test)[:,1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_smot = np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23488851786018133"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_smot.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'SMOTE')\n",
      "(1, 'SMOTE')\n",
      "(2, 'SMOTE')\n",
      "(3, 'SMOTE')\n",
      "(4, 'SMOTE')\n",
      "(5, 'SMOTE')\n",
      "(6, 'SMOTE')\n",
      "(7, 'SMOTE')\n",
      "(8, 'SMOTE')\n",
      "(9, 'SMOTE')\n"
     ]
    }
   ],
   "source": [
    "p1 = make_pipeline(\n",
    "    RandomOverSampler(),\n",
    "    ExtractFeatures(cols_for_lr, df_train_copy.columns),\n",
    "    StandardScaler(),\n",
    "    # ExtractFeatures(cols_for_random_forest, X_train.columns),\n",
    "    # RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=None, n_jobs=4)\n",
    "    LogisticRegression(n_jobs=-1)\n",
    ")\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, 'RandomOS')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_train_copy, df_train_copy['target'], test_size=0.2\n",
    "    )\n",
    "    p1.fit(X_train, y_train)\n",
    "    acc.append(gini_normalized(y_test, p1.predict_proba(X_test)[:,1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_random_os = np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2433658426353956"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_random_os.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'adasyn')\n",
      "(1, 'adasyn')\n",
      "(2, 'adasyn')\n",
      "(3, 'adasyn')\n",
      "(4, 'adasyn')\n",
      "(5, 'adasyn')\n",
      "(6, 'adasyn')\n",
      "(7, 'adasyn')\n",
      "(8, 'adasyn')\n",
      "(9, 'adasyn')\n"
     ]
    }
   ],
   "source": [
    "p1 = make_pipeline(\n",
    "    ADASYN(),\n",
    "    ExtractFeatures(cols_for_lr, df_train_copy.columns),\n",
    "    StandardScaler(),\n",
    "    # ExtractFeatures(cols_for_random_forest, X_train.columns),\n",
    "    # RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=None, n_jobs=4)\n",
    "    LogisticRegression(n_jobs=-1)\n",
    ")\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, 'adasyn')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_train_copy, df_train_copy['target'], test_size=0.2\n",
    "    )\n",
    "    p1.fit(X_train, y_train)\n",
    "    acc.append(gini_normalized(y_test, p1.predict_proba(X_test)[:,1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_random_ada = np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24147230463576355"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_random_ada.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'randomus')\n",
      "(1, 'randomus')\n",
      "(2, 'randomus')\n",
      "(3, 'randomus')\n",
      "(4, 'randomus')\n",
      "(5, 'randomus')\n",
      "(6, 'randomus')\n",
      "(7, 'randomus')\n",
      "(8, 'randomus')\n",
      "(9, 'randomus')\n"
     ]
    }
   ],
   "source": [
    "p1 = make_pipeline(\n",
    "    RandomUnderSampler(),\n",
    "    ExtractFeatures(cols_for_lr, df_train_copy.columns),\n",
    "    StandardScaler(),\n",
    "    # ExtractFeatures(cols_for_random_forest, X_train.columns),\n",
    "    # RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=None, n_jobs=4)\n",
    "    LogisticRegression(n_jobs=-1)\n",
    ")\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, 'randomus')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_train_copy, df_train_copy['target'], test_size=0.2\n",
    "    )\n",
    "    p1.fit(X_train, y_train)\n",
    "    acc.append(gini_normalized(y_test, p1.predict_proba(X_test)[:,1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_random_us = np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24620052221528463"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_random_us.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'randomus')\n",
      "(1, 'randomus')\n",
      "(2, 'randomus')\n",
      "(3, 'randomus')\n",
      "(4, 'randomus')\n",
      "(5, 'randomus')\n",
      "(6, 'randomus')\n",
      "(7, 'randomus')\n",
      "(8, 'randomus')\n",
      "(9, 'randomus')\n"
     ]
    }
   ],
   "source": [
    "p1 = make_pipeline(\n",
    "    SMOTEENN(),\n",
    "    ExtractFeatures(cols_for_lr, df_train_copy.columns),\n",
    "    StandardScaler(),\n",
    "    # ExtractFeatures(cols_for_random_forest, X_train.columns),\n",
    "    # RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=None, n_jobs=4)\n",
    "    LogisticRegression(n_jobs=-1)\n",
    ")\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, 'smotten')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_train_copy, df_train_copy['target'], test_size=0.2\n",
    "    )\n",
    "    p1.fit(X_train, y_train)\n",
    "    acc.append(gini_normalized(y_test, p1.predict_proba(X_test)[:,1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_smotten = np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23725155204523718"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_smotten.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada</th>\n",
       "      <th>oversamp</th>\n",
       "      <th>undersample</th>\n",
       "      <th>smote</th>\n",
       "      <th>smotten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.253428</td>\n",
       "      <td>0.237684</td>\n",
       "      <td>0.251830</td>\n",
       "      <td>0.235577</td>\n",
       "      <td>0.244569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.240331</td>\n",
       "      <td>0.261890</td>\n",
       "      <td>0.240529</td>\n",
       "      <td>0.244412</td>\n",
       "      <td>0.239521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.237150</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.250779</td>\n",
       "      <td>0.226790</td>\n",
       "      <td>0.241326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.235944</td>\n",
       "      <td>0.245608</td>\n",
       "      <td>0.245333</td>\n",
       "      <td>0.236361</td>\n",
       "      <td>0.237616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.236073</td>\n",
       "      <td>0.224721</td>\n",
       "      <td>0.258767</td>\n",
       "      <td>0.242088</td>\n",
       "      <td>0.237371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.238398</td>\n",
       "      <td>0.250222</td>\n",
       "      <td>0.240580</td>\n",
       "      <td>0.217712</td>\n",
       "      <td>0.236523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.235966</td>\n",
       "      <td>0.242819</td>\n",
       "      <td>0.242224</td>\n",
       "      <td>0.242894</td>\n",
       "      <td>0.241098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.252922</td>\n",
       "      <td>0.245038</td>\n",
       "      <td>0.235973</td>\n",
       "      <td>0.240498</td>\n",
       "      <td>0.233180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.237245</td>\n",
       "      <td>0.231596</td>\n",
       "      <td>0.253828</td>\n",
       "      <td>0.234168</td>\n",
       "      <td>0.247017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.247267</td>\n",
       "      <td>0.249281</td>\n",
       "      <td>0.242164</td>\n",
       "      <td>0.228385</td>\n",
       "      <td>0.214295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ada  oversamp  undersample     smote   smotten\n",
       "0  0.253428  0.237684     0.251830  0.235577  0.244569\n",
       "1  0.240331  0.261890     0.240529  0.244412  0.239521\n",
       "2  0.237150  0.244800     0.250779  0.226790  0.241326\n",
       "3  0.235944  0.245608     0.245333  0.236361  0.237616\n",
       "4  0.236073  0.224721     0.258767  0.242088  0.237371\n",
       "5  0.238398  0.250222     0.240580  0.217712  0.236523\n",
       "6  0.235966  0.242819     0.242224  0.242894  0.241098\n",
       "7  0.252922  0.245038     0.235973  0.240498  0.233180\n",
       "8  0.237245  0.231596     0.253828  0.234168  0.247017\n",
       "9  0.247267  0.249281     0.242164  0.228385  0.214295"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    pd.Series(acc_random_ada),\n",
    "    pd.Series(acc_random_os),\n",
    "    pd.Series(acc_random_us),\n",
    "    pd.Series(acc_smot),\n",
    "    pd.Series(acc_smotten)\n",
    "], axis=1, keys=['ada', 'oversamp', 'undersample', 'smote', 'smotten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>averaged</th>\n",
       "      <th>bc_gini</th>\n",
       "      <th>lr_gini</th>\n",
       "      <th>rf_gini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.255151</td>\n",
       "      <td>0.157132</td>\n",
       "      <td>0.252415</td>\n",
       "      <td>0.156841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.249068</td>\n",
       "      <td>0.151016</td>\n",
       "      <td>0.252121</td>\n",
       "      <td>0.155272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.230510</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.229635</td>\n",
       "      <td>0.163730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.235002</td>\n",
       "      <td>0.155562</td>\n",
       "      <td>0.228198</td>\n",
       "      <td>0.140790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.238146</td>\n",
       "      <td>0.140436</td>\n",
       "      <td>0.240538</td>\n",
       "      <td>0.149440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.243857</td>\n",
       "      <td>0.144852</td>\n",
       "      <td>0.243409</td>\n",
       "      <td>0.159457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.239681</td>\n",
       "      <td>0.156499</td>\n",
       "      <td>0.236141</td>\n",
       "      <td>0.163166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.239429</td>\n",
       "      <td>0.153807</td>\n",
       "      <td>0.231204</td>\n",
       "      <td>0.160510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.254107</td>\n",
       "      <td>0.159629</td>\n",
       "      <td>0.256301</td>\n",
       "      <td>0.164851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.242820</td>\n",
       "      <td>0.150408</td>\n",
       "      <td>0.247106</td>\n",
       "      <td>0.158329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   averaged   bc_gini   lr_gini   rf_gini\n",
       "0  0.255151  0.157132  0.252415  0.156841\n",
       "1  0.249068  0.151016  0.252121  0.155272\n",
       "2  0.230510  0.147700  0.229635  0.163730\n",
       "3  0.235002  0.155562  0.228198  0.140790\n",
       "4  0.238146  0.140436  0.240538  0.149440\n",
       "5  0.243857  0.144852  0.243409  0.159457\n",
       "6  0.239681  0.156499  0.236141  0.163166\n",
       "7  0.239429  0.153807  0.231204  0.160510\n",
       "8  0.254107  0.159629  0.256301  0.164851\n",
       "9  0.242820  0.150408  0.247106  0.158329"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(acc_with_sub_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'random undersampling')\n",
      "(1, 'random undersampling')\n",
      "(2, 'random undersampling')\n",
      "(3, 'random undersampling')\n",
      "(4, 'random undersampling')\n",
      "(5, 'random undersampling')\n",
      "(6, 'random undersampling')\n",
      "(7, 'random undersampling')\n",
      "(8, 'random undersampling')\n",
      "(9, 'random undersampling')\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsemble\n",
    "acc = []\n",
    "for i in range(10):\n",
    "    print(i, 'random undersampling')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_train_copy, df_train_copy['target'], test_size=0.2\n",
    "    )\n",
    "    ee = EasyEnsemble(n_subsets=20)\n",
    "    X_resampled, y_resampled = ee.fit_sample(X_train, y_train)\n",
    "    p1 = make_pipeline(    \n",
    "        ExtractFeatures(cols_for_lr, df_train_copy.columns),\n",
    "        StandardScaler(),\n",
    "        # ExtractFeatures(cols_for_random_forest, X_train.columns),\n",
    "        # RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=None, n_jobs=4)\n",
    "        LogisticRegression(n_jobs=-1)\n",
    "    )\n",
    "    m = []\n",
    "    for j in range(X_resampled.shape[0]):\n",
    "        m.append(p1.fit(X_resampled[j], y_resampled[j]))\n",
    "\n",
    "    r = []\n",
    "    for j in m:\n",
    "        r.append(j.predict(X_test))\n",
    "    ans = np.array(r).mean(axis=0)    \n",
    "    acc.append(gini_normalized(y_test, ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16472553786600316,\n",
       " 0.19146966415267491,\n",
       " 0.1772546486143953,\n",
       " 0.17172927882928207,\n",
       " 0.16447852716697131,\n",
       " 0.17018646254723979,\n",
       " 0.18827503245692345,\n",
       " 0.17236914391368621,\n",
       " 0.18193425571658708,\n",
       " 0.17623813149013859]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "p1 = make_pipeline(\n",
    "    ExtractFeatures(all_cols, df_train_copy.columns),\n",
    "    BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "        ratio='auto',\n",
    "        n_estimators=100,\n",
    "        replacement=False,\n",
    "    ) \n",
    ")\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, 'bagging balanced')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_train_copy, df_train_copy['target'], test_size=0.2\n",
    "    )\n",
    "    p1.fit(X_train, y_train)\n",
    "    acc.append(gini_normalized(y_test, p1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1693896728459835,\n",
       " 0.16618195722301302,\n",
       " 0.18371921123840745,\n",
       " 0.16490023213808078,\n",
       " 0.17476471469413715,\n",
       " 0.16081644773843329,\n",
       " 0.16542185049146743,\n",
       " 0.17106598879042093,\n",
       " 0.16310255002402649,\n",
       " 0.17469009764631929]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'bagging balanced')\n",
      "(1, 'bagging balanced')\n",
      "(2, 'bagging balanced')\n",
      "(3, 'bagging balanced')\n",
      "(4, 'bagging balanced')\n",
      "(5, 'bagging balanced')\n",
      "(6, 'bagging balanced')\n",
      "(7, 'bagging balanced')\n",
      "(8, 'bagging balanced')\n",
      "(9, 'bagging balanced')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "p1 = make_pipeline(\n",
    "    ExtractFeatures(cols_for_lr, df_train_copy.columns),\n",
    "    BalancedBaggingClassifier(base_estimator=LogisticRegression(),\n",
    "        ratio='auto',\n",
    "        n_estimators=100,\n",
    "        replacement=False,\n",
    "    ) \n",
    ")\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, 'bagging balanced')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_train_copy, df_train_copy['target'], test_size=0.2\n",
    "    )\n",
    "    p1.fit(X_train, y_train)\n",
    "    acc.append(gini_normalized(y_test, p1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17421833087875949,\n",
       " 0.1657204548527598,\n",
       " 0.17218184557625865,\n",
       " 0.17476176883100758,\n",
       " 0.18068998999732605,\n",
       " 0.18658202836245627,\n",
       " 0.1758742111837176,\n",
       " 0.1707358348557296,\n",
       " 0.174572490215683,\n",
       " 0.18025772703885518]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
